{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.8"
    },
    "colab": {
      "name": "TP2_bandits_2020.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/EmmanuelJhno/Reinforcement_learning/blob/master/TP2_bandits_2020.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_ZgafVXsagxj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d2O8v3N5agyN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class BernoulliBanditEnv(object): \n",
        "    # Class that defines the environment with reward 0 and 1 with probability p.\n",
        "\n",
        "    def __init__(self, num_arms=10, p=None):\n",
        "        self.num_arms = num_arms\n",
        "        self.actions = np.arange(num_arms)     # define set of actions\n",
        "\n",
        "        if len(p)==1:\n",
        "            self.p = np.random.beta(0.5, 0.5, size=num_arms)\n",
        "        elif len(p) == num_arms:\n",
        "            self.p = p\n",
        "        else:\n",
        "            raise Exception('Number of probabilities ({}) does not correspond to number of arms ({}).'.format(len(q), num_arms))\n",
        "        self.best_action = np.argmax(self.p)   # Best action given env\n",
        "\n",
        "    def reward(self, action):\n",
        "        return np.random.binomial(1, p=self.p[action])\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zLD0Udtpagyh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Agent(object):\n",
        "    # Class which defines the agent. Each aganet has a decision rule and a learning rule.\n",
        "    \n",
        "    def __init__(self, learning_rule, decision_rule, param=None):\n",
        "        self.decision_rule = decision_rule\n",
        "        self.learning_rule = learning_rule\n",
        "\n",
        "        if decision_rule == \"epsilon-greedy\":\n",
        "            self.epsilon = param[\"epsilon\"]\n",
        "        \n",
        "        if decision_rule == \"UCB\":\n",
        "            self.UCB_param = param[\"UCB_param\"]\n",
        "    \n",
        "    def environment(self, env, init_q):\n",
        "        # initialize environment\n",
        "        self.env = env                                  \n",
        "        self.k = env.num_arms                           \n",
        "        self.actions = np.arange(self.k)                \n",
        "        self.act_count = np.zeros(self.k)               \n",
        "        self.iteration = 0     \n",
        "        if self.learning_rule == \"BayesianBetaPrior\":\n",
        "            self.alpha = np.random.uniform(size=self.k)\n",
        "            self.beta = np.random.uniform(size=self.k)\n",
        "        if len(init_q) == self.k:\n",
        "            self.q_estimate = init_q\n",
        "        else:\n",
        "            raise Exception('Number of initial values ({}) does not correspond to number of arms ({}).'.format(len(init_q), self.k))\n",
        "\n",
        "    def learn(self, a, r):\n",
        "        # given action and reward, update value function.\n",
        "        if self.learning_rule == \"averaging\":\n",
        "            self.q_estimate[a] += 1/self.act_count[a] * (r - self.q_estimate[a])\n",
        "            \n",
        "        if self.learning_rule == \"BayesianBetaPrior\":\n",
        "            self.alpha[a] += r\n",
        "            self.beta[a] += 1 - r \n",
        "            \n",
        "    def act(self):\n",
        "        # action.\n",
        "        self.iteration += 1 \n",
        "        if self.decision_rule == \"greedy\":\n",
        "            # COMPLETE\n",
        "            pass\n",
        "\n",
        "        if self.decision_rule == \"epsilon-greedy\":\n",
        "            # COMPLETE\n",
        "            pass\n",
        "        \n",
        "        if self.decision_rule == \"UCB\":\n",
        "            # COMPLETE\n",
        "            pass\n",
        "        if self.decision_rule == \"Thompson\":\n",
        "            # COMPLETE\n",
        "            pass\n",
        "        self.act_count[selected_action] += 1\n",
        "        return selected_action"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_JeHwuC4agyx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def simulateBandits(agents, narms, initp=None, initq=None, repetitions=1000, N=100):\n",
        "    # function that simulates the agents behaviour\n",
        "    # agents is a list of agents.\n",
        "    \n",
        "    rewards = np.zeros((len(agents), repetitions, N))\n",
        "    bestarm = np.zeros((len(agents), repetitions, N))\n",
        "    for i, agent in enumerate(agents):\n",
        "        for j in np.arange(repetitions):\n",
        "            environment = BernoulliBanditEnv(num_arms=narms, p=initp)\n",
        "            agent.environment(environment, initq if not(initq == None) else np.zeros(narms))\n",
        "            for n in np.arange(N):\n",
        "                a = agent.act()\n",
        "                r = environment.reward(a)\n",
        "                agent.learn(a, r)\n",
        "                rewards[i, j, n] = r\n",
        "                bestarm[i, j, n] = 1 if a == environment.best_action else 0\n",
        "    \n",
        "    return np.squeeze(np.mean(rewards, axis=1)), np.squeeze(np.mean(bestarm, axis=1))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wJABFA1aagy_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def plot_results(agents, actions, rewards):\n",
        "    # COMPLETE\n",
        "    pass"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L89MIpP7agzO",
        "colab_type": "text"
      },
      "source": [
        "# Exercises:\n",
        "\n",
        "1) COMPLETE the code where it says \"COMPLETE\".\n",
        "\n",
        "2) Do simulations for a bandit with 2 arms P = [0.4, 0.8] for each of the mentionned decision rule and plot the corresponding  mean reward; the mean cumulative reward and the percentage of times the best arm was elected as time goes by. Interpret. \n",
        "\n",
        "\n",
        "3) Do simulations with a bandit with 10 arms (P = [0.2, 0.2, 0.4, 0.2, 0.2, 0.2, 0.2, 0.8, 0.2, 0.2]). Plot the corresponding  mean reward; the mean cumulative reward and the percentage of times the best arm was elected as time goes by. Interpret.  \n",
        "\n",
        "4) Study the dependence of the hyperparameter epsilon in the decision rule epsilon-greedy.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nlbM3lVbagzT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}