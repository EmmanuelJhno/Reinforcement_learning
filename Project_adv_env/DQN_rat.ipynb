{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras import backend as K\n",
    "import numpy as np\n",
    "import io\n",
    "import base64\n",
    "from IPython.display import HTML\n",
    "#import skvideo\n",
    "#skvideo.setFFmpegPath(\"/usr/local/bin\")\n",
    "import skvideo.io\n",
    "import cv2\n",
    "import json\n",
    "\n",
    "from keras.models import Sequential, model_from_json\n",
    "from keras.layers.core import Dense, Dropout, Flatten\n",
    "from keras.optimizers import sgd, Adam\n",
    "from keras.layers import Conv2D, MaxPooling2D, Activation, AveragePooling2D,Reshape,BatchNormalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project on Deep Reinforcement Learning and environment modifications"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Notations__: $E_p$ is the expectation under probability $p$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Context"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The environment, the agent and the game"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```Environment``` is an abstract class that represents the states, rewards, and actions to obtain the new state."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The method ```act``` allows to act on the environment at a given state $s_t$ (stored internally), via action $a_t$. The method will return the new state $s_{t+1}$, the reward $r(s_{t},a_{t})$ and determines if $t\\leq T$ (*game_over*).\n",
    "\n",
    "The method ```reset``` simply reinitializes the environment to a random state $s_0$.\n",
    "\n",
    "The method ```draw``` displays the current state $s_t$ (this is useful to check the behavior of the Agent).\n",
    "\n",
    "We modelize $s_t$ as a tensor, while $a_t$ is an integer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of the ```Agent``` is to interact with the ```Environment``` by proposing actions $a_t$ obtained from a given state $s_t$ to attempt to maximize its __reward__ $r(s_t,a_t)$. We propose the following abstract class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent(object):\n",
    "    def __init__(self, epsilon=0.1, n_action=4):\n",
    "        self.epsilon = epsilon\n",
    "        self.n_action = n_action\n",
    "        self.vis = 2\n",
    "    \n",
    "    def set_epsilon(self,e):\n",
    "        self.epsilon = e\n",
    "\n",
    "    def act(self,s,train=True):\n",
    "        \"\"\" This function should return the next action to do:\n",
    "        an integer between 0 and 4 (not included) with a random exploration of epsilon\"\"\"\n",
    "        if train:\n",
    "            if np.random.rand() <= self.epsilon:\n",
    "                a = np.random.randint(0, self.n_action, size=1)[0]\n",
    "            else:\n",
    "                a = self.learned_act(s)\n",
    "        else: # in some cases, this can improve the performance.. remove it if poor performances\n",
    "            a = self.learned_act(s)\n",
    "\n",
    "        return a\n",
    "\n",
    "    def learned_act(self,s):\n",
    "        \"\"\" Act via the policy of the agent, from a given state s\n",
    "        it proposes an action a\"\"\"\n",
    "        pass\n",
    "\n",
    "    def reinforce(self, s, n_s, a, r, game_over_):\n",
    "        \"\"\" This function is the core of the learning algorithm. \n",
    "        It takes as an input the current state s_, the next state n_s_\n",
    "        the action a_ used to move from s_ to n_s_ and the reward r_.\n",
    "        \n",
    "        Its goal is to learn a policy.\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    def save(self):\n",
    "        \"\"\" This function returns basic stats if applicable: the\n",
    "        loss and/or the model\"\"\"\n",
    "        pass\n",
    "\n",
    "    def load(self):\n",
    "        \"\"\" This function allows to restore a model\"\"\"\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# The game, *eat cheese*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A rat runs on an island and tries to eat as much as possible. The island is subdivided into $N\\times N$ cells, in which there are cheese (+0.5) and poisonous cells (-1). The rat has a visibility of 2 cells (thus it can see $5^2$ cells). The rat is given a time $T$ to accumulate as much food as possible. It can perform 4 actions: going up, down, left, right. \n",
    "\n",
    "The goal is to code an agent to solve this task that will learn by trial and error. We propose the following environment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Environment(object):\n",
    "    def __init__(self, grid_size=20, max_time=200, temperature=0.3, immob_penalty=0):\n",
    "        grid_size = grid_size+4\n",
    "        self.grid_size = grid_size\n",
    "        self.max_time = max_time\n",
    "        self.temperature = temperature \n",
    "        self.immob_penalty = immob_penalty\n",
    "        self.vis = 2\n",
    "        # this will impact the number of positive and negative rewards in the environment\n",
    "\n",
    "        # board on which one plays\n",
    "        self.board = np.zeros((grid_size,grid_size)) # possible reward in each location on the board\n",
    "        self.position = np.zeros((grid_size,grid_size)) # the very position of the rat at time t\n",
    "        self.malus_position = np.zeros((self.grid_size, self.grid_size)) # recording malus rewards on visited locations\n",
    "        \n",
    "        # coordinate of the rat\n",
    "        self.x = 0\n",
    "        self.y = 1\n",
    "\n",
    "        # self time\n",
    "        self.t = 0\n",
    "\n",
    "        self.scale=16\n",
    "\n",
    "        self.to_draw = np.zeros((max_time+2, grid_size*self.scale, grid_size*self.scale, 3))\n",
    "        \n",
    "\n",
    "    def draw(self,e):\n",
    "        skvideo.io.vwrite('runs/' + str(e) + '.mp4', self.to_draw)\n",
    "\n",
    "    def get_frame(self,t):\n",
    "        b = np.zeros((self.grid_size,self.grid_size,3))+128\n",
    "        b[self.board>0, 0] = 256\n",
    "        b[self.big_bonus>0, 1] = 256\n",
    "        b[self.board<0, 2] = 256\n",
    "        b[self.x,self.y,:] = 256\n",
    "        b[-4:,:,:]=0\n",
    "        b[:,-4:,:]=0\n",
    "        b[:4,:,:]=0\n",
    "        b[:,:4,:]=0\n",
    "        \n",
    "        b = cv2.resize(b, None, fx=self.scale, fy=self.scale, interpolation=cv2.INTER_NEAREST)\n",
    "\n",
    "        self.to_draw[t,:,:,:] = b\n",
    "        \n",
    "        \n",
    "    def act(self, action, train=True):\n",
    "        \"\"\"This function returns the new state, reward and decides if the\n",
    "        game ends.\"\"\"\n",
    "\n",
    "        self.get_frame(int(self.t))\n",
    "\n",
    "        self.position = np.zeros((self.grid_size, self.grid_size))\n",
    "\n",
    "        self.position[0:4, :] = -1\n",
    "        self.position[:, 0:4] = -1\n",
    "        self.position[-4:, :] = -1\n",
    "        self.position[:, -4:] = -1\n",
    "\n",
    "        self.position[self.x, self.y] = 1\n",
    "        if action == 0:\n",
    "            if self.x == self.grid_size-5:\n",
    "                self.x = self.x - 1\n",
    "            else:\n",
    "                self.x = self.x + 1\n",
    "        elif action == 1:\n",
    "            if self.x == 4:\n",
    "                self.x = self.x + 1\n",
    "            else:\n",
    "                self.x = self.x - 1\n",
    "        elif action == 2:\n",
    "            if self.y == self.grid_size - 5:\n",
    "                self.y = self.y - 1\n",
    "            else:\n",
    "                self.y = self.y + 1\n",
    "        elif action == 3:\n",
    "            if self.y == 4:\n",
    "                self.y = self.y + 1\n",
    "            else:\n",
    "                self.y = self.y - 1\n",
    "        else:\n",
    "            RuntimeError('Error: action not recognized')\n",
    "\n",
    "        self.t = self.t + 1\n",
    "        \n",
    "        ### Then before giving the reward, we give the environment the possibility to change and act differently\n",
    "        self.modify_board() # Everything needed can be found in self.x, self.y, self.board, self.t, etc...\n",
    "        \n",
    "        reward = 0\n",
    "        if train:\n",
    "            reward += self.malus_position[self.x, self.y]\n",
    "            reward += self.big_bonus[self.x, self.y]\n",
    "            \n",
    "        self.malus_position[self.x, self.y] = - self.immob_penalty\n",
    "        self.big_bonus[self.x, self.y] = 0\n",
    "        \n",
    "        reward += self.board[self.x, self.y]\n",
    "        \n",
    "        self.board[self.x, self.y] = 0\n",
    "        game_over = self.t > self.max_time\n",
    "        state = np.concatenate((self.malus_position.reshape(self.grid_size, self.grid_size,1),\n",
    "                                self.board.reshape(self.grid_size, self.grid_size,1),\n",
    "                                self.position.reshape(self.grid_size, self.grid_size,1),\n",
    "                                self.big_bonus.reshape(self.grid_size, self.grid_size,1)),axis=2)\n",
    "        state = state[(self.x-self.vis):(self.x+1+self.vis),(self.y-self.vis):(self.y+1+self.vis),:]\n",
    "\n",
    "        state_troncated = np.zeros((9,9,state.shape[2]))\n",
    "        if self.vis == 4 :\n",
    "            state_troncated = state\n",
    "        else:\n",
    "            state_troncated[(4-self.vis):(self.vis-4),(4-self.vis):(self.vis-4),:]=state\n",
    "        \n",
    "        return state_troncated, reward, game_over\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"This function resets the game and returns the initial state\"\"\"\n",
    "        self.vis = 2\n",
    "        self.x = np.random.randint(5, self.grid_size-5, size=1)[0]\n",
    "        self.y = np.random.randint(5, self.grid_size-5, size=1)[0]\n",
    "\n",
    "        self.malus_position = np.zeros((self.grid_size, self.grid_size))\n",
    "        self.big_bonus = np.zeros((self.grid_size, self.grid_size))\n",
    "        \n",
    "        island_size = self.grid_size - 8\n",
    "        grid_bonus = 0.5*np.random.binomial(1, self.temperature, size=island_size**2)\n",
    "        grid_bonus = grid_bonus.reshape(island_size, island_size)\n",
    "        bonus = np.zeros((self.grid_size, self.grid_size))\n",
    "        bonus[4:-4,4:-4] = grid_bonus\n",
    "\n",
    "        grid_malus = -1.0*np.random.binomial(1, self.temperature, size=island_size**2)\n",
    "        grid_malus = grid_malus.reshape(island_size, island_size)\n",
    "        malus = np.zeros((self.grid_size, self.grid_size))\n",
    "        malus[4:-4,4:-4] = grid_malus\n",
    "\n",
    "        self.to_draw = np.zeros((self.max_time+2, self.grid_size*self.scale, self.grid_size*self.scale, 3))\n",
    "\n",
    "\n",
    "        malus[bonus>0] = 0\n",
    "\n",
    "        self.board = bonus + malus\n",
    "\n",
    "        self.position = np.zeros((self.grid_size, self.grid_size))\n",
    "        self.position[:4, :] = -1\n",
    "        self.position[:, :4] = -1\n",
    "        self.position[-4:,:] = -1\n",
    "        self.position[:,-4:] = -1\n",
    "        self.board[self.x,self.y] = 0 # since we assume that we don't consider the first state in the reward,\n",
    "        self.malus_position[self.x, self.y] = - self.immob_penalty # we can consider the original position as visited (or not ?)\n",
    "        self.t = 0\n",
    "\n",
    "        state = np.concatenate((self.malus_position.reshape(self.grid_size, self.grid_size,1),\n",
    "                                self.board.reshape(self.grid_size, self.grid_size,1),\n",
    "                                self.position.reshape(self.grid_size, self.grid_size,1),\n",
    "                                self.big_bonus.reshape(self.grid_size, self.grid_size,1)),axis=2)\n",
    "\n",
    "        state = state[self.x - 2:self.x + 3, self.y - 2:self.y + 3, :]\n",
    "        \n",
    "        state_troncated = np.zeros((9,9,state.shape[2]))\n",
    "        if self.vis == 4 :\n",
    "            state_troncated = state\n",
    "        else:\n",
    "            state_troncated[(4-self.vis):(self.vis-4),(4-self.vis):(self.vis-4),:]=state\n",
    "       \n",
    "        return state_troncated\n",
    "    \n",
    "    def modify_board(self):\n",
    "        \"\"\"This function modifies the board itself. It should use self.t and self.board\"\"\"\n",
    "        pass\n",
    "    \n",
    "    def modify_visibility(self, win, lose):\n",
    "        \"\"\"This function modifies the visibility of the Agent. It should use self.vis\"\"\"\n",
    "        pass\n",
    "    \n",
    "    def add_big_reward(self, win):\n",
    "        \"\"\"This function modifies the visibility of the Agent. It should use self.vis\"\"\"\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's define now the hyper parameters :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters\n",
    "size = 19\n",
    "T=500\n",
    "temperature=0.3\n",
    "epochs_train=10 # set small when debugging\n",
    "epochs_test=10 # set small when debugging\n",
    "\n",
    "# display videos\n",
    "def display_videos(name):\n",
    "    video = io.open('runs/'+name, 'r+b').read()\n",
    "    encoded = base64.b64encode(video)\n",
    "    return '''<video alt=\"test\" controls>\n",
    "                <source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\" />\n",
    "             </video>'''.format(encoded.decode('ascii'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now define the training loop and the test loop :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(agent, env,epoch, eps_decay=None, eps_step=None, freq_video=10, prefix=''):\n",
    "    # Number of won games\n",
    "    score = 0\n",
    "    loss = 0\n",
    "    \n",
    "    name = prefix\n",
    "    name = name + '_explore_' if eps_step and eps_decay else name\n",
    "    \n",
    "    eps = agent.epsilon\n",
    "\n",
    "    for e in range(epoch):\n",
    "        \n",
    "        if eps_step and eps_decay and e!=0 and e%eps_step==0:\n",
    "            eps = eps*eps_decay\n",
    "            agent.set_epsilon(eps)\n",
    "        # At each epoch, we restart to a fresh game and get the initial state\n",
    "        state = env.reset()\n",
    "        # This assumes that the games will terminate\n",
    "        game_over = False\n",
    "        \n",
    "        win = 0\n",
    "        lose = 0\n",
    "\n",
    "        while not game_over:\n",
    "            # The agent performs an action\n",
    "            action = agent.act(state)\n",
    "            \n",
    "            # Apply an action to the environment, get the next state, the reward\n",
    "            # and if the games end\n",
    "            prev_state = state\n",
    "            \n",
    "            state, reward, game_over = env.act(action, train=True)\n",
    "            \n",
    "            # Update the counters\n",
    "            # We modify this interval because we only care about the true reward \n",
    "            # and not the penalty of coming back to visited places. This penalty aims at helping the algorithm\n",
    "            # itself but shouldn't be considered in the win/lose counts\n",
    "            win_before = win\n",
    "            if reward > 0:\n",
    "                win += reward\n",
    "            elif reward != -env.immob_penalty:\n",
    "                lose -= reward\n",
    "            \n",
    "            # Apply the reinforcement strategy\n",
    "            loss = agent.reinforce(prev_state, state, action, reward, game_over)\n",
    "            \n",
    "            env.modify_visibility(win, lose)\n",
    "            \n",
    "            if win_before < win :\n",
    "                env.add_big_reward(win)\n",
    "            \n",
    "            agent.vis = env.vis\n",
    "            \n",
    "        # Save as a mp4\n",
    "        if (e+1) % freq_video == 0:\n",
    "            env.draw(name+str(e+1))\n",
    "\n",
    "        # Update stats\n",
    "        score += win-lose\n",
    "        \n",
    "        win_max = np.sum(np.where(env.to_draw[0, ::env.scale, ::env.scale, 0]==256, 1.0, 0))\n",
    "        lose_max = np.sum(np.where(env.to_draw[0, ::env.scale, ::env.scale, 2]==256, 1.0, 0))\n",
    "\n",
    "        print(\"Epoch {0:03d}/{1:03d} | Loss {2:.4f} | Win/lose count {3}({4:.1f}%)/{5}({6:.1f}%) ({7:.1f})\"\n",
    "              .format(e+1, epoch, loss, win, 100*win/win_max, lose, 100*lose/lose_max, win-lose))\n",
    "        agent.save(name_weights=name+'_model.h5',name_model=name+'_model.json')\n",
    "        \n",
    "        \n",
    "#------------------------------------------------------------------------------------------------\n",
    "\n",
    "def test(agent,env,epochs,prefix=''):\n",
    "    # Number of won games\n",
    "    score = 0\n",
    "        \n",
    "    for e in range(1,epochs+1):\n",
    "        # At each epoch, we restart to a fresh game and get the initial state\n",
    "        \n",
    "        state = env.reset()\n",
    "        \n",
    "        # This assumes that the games will end\n",
    "        game_over = False\n",
    "\n",
    "        win = 0\n",
    "        lose = 0\n",
    "\n",
    "        while not game_over:\n",
    "            # The agent performs an action\n",
    "            action = agent.act(state)\n",
    "\n",
    "            # Apply an action to the environment, get the next state, the reward\n",
    "            # and if the games end\n",
    "            prev_state = state\n",
    "            state, reward, game_over = env.act(action)\n",
    "\n",
    "            # Update the counters\n",
    "            if reward > 0:\n",
    "                win = win + reward\n",
    "            if reward < -0.1:\n",
    "                lose = lose - reward\n",
    "\n",
    "        # Save as a mp4\n",
    "        env.draw(prefix+str(e))\n",
    "\n",
    "        # Update stats\n",
    "        score = score + win-lose\n",
    "        \n",
    "        win_max = np.sum(np.where(env.to_draw[0, ::env.scale, ::env.scale, 0]==256, 1.0, 0))\n",
    "        lose_max = np.sum(np.where(env.to_draw[0, ::env.scale, ::env.scale, 2]==256, 1.0, 0))\n",
    "\n",
    "        print(\"Win/lose count {}({:.1f}%)/{}({:.1f}%). Average score ({:.1f})\"\n",
    "              .format(win, 100*win/win_max, lose, 100*lose/lose_max, score/e))\n",
    "    print('Final score: '+str(score/epochs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomAgent(Agent):\n",
    "    def __init__(self):\n",
    "        super(RandomAgent, self).__init__()\n",
    "\n",
    "    def learned_act(self, s):\n",
    "        return np.random.randint(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try it out ! *NB : This random agent cannot be trained so we test it directly*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Win/lose count 21.0(28.0%)/31.0(57.4%). Average score (-10.0)\n",
      "Win/lose count 19.0(26.8%)/28.0(58.3%). Average score (-9.5)\n",
      "Win/lose count 22.0(33.3%)/34.0(63.0%). Average score (-10.3)\n",
      "Win/lose count 19.0(30.2%)/15.0(31.9%). Average score (-6.8)\n",
      "Win/lose count 17.5(27.3%)/34.0(66.7%). Average score (-8.7)\n",
      "Win/lose count 27.0(33.3%)/33.0(70.2%). Average score (-8.2)\n",
      "Win/lose count 17.0(27.0%)/23.0(44.2%). Average score (-7.9)\n",
      "Win/lose count 21.0(26.2%)/24.0(54.5%). Average score (-7.3)\n",
      "Win/lose count 18.0(22.5%)/18.0(47.4%). Average score (-6.5)\n",
      "Win/lose count 20.5(28.1%)/29.0(64.4%). Average score (-6.7)\n",
      "Final score: -6.7\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<video alt=\"test\" controls>\n",
       "                <source src=\"data:video/mp4;base64,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\" type=\"video/mp4\" />\n",
       "             </video>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize the game\n",
    "env = Environment(grid_size=size, max_time=T, temperature=temperature)\n",
    "\n",
    "# Initialize the agent!\n",
    "agent = RandomAgent()\n",
    "\n",
    "test(agent, env, epochs_test, prefix='random')\n",
    "HTML(display_videos('random1.mp4'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> A random agent is not great and performs really badly... Let's head to a more suitable approach when actually learning something by reinforcement. However, this agent will be useful to draw comparisons when modifying the environment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## DQN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "The DQN-learning algorithm relies on these derivations to train the parameters $\\theta$ of a Deep Neural Network:\n",
    "\n",
    "1. At the state $s_t$, select the action $a_t$ with best reward using $Q_t$ and store the results;\n",
    "\n",
    "2. Obtain the new state $s_{t+1}$ from the environment $p$;\n",
    "\n",
    "3. Store $(s_t,a_t,s_{t+1})$;\n",
    "\n",
    "4. Obtain $Q_{t+1}$ by minimizing  $\\mathcal{L}$ from a recovered batch from the previously stored results.\n",
    "\n",
    "***\n",
    "We will need a class ```Memory``` that stores moves (in a replay buffer) via ```remember``` and provides a ```random_access``` to these. A maximum memory size is required to avoid side effects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Memory(object):\n",
    "    def __init__(self, max_memory=100):\n",
    "        self.max_memory = max_memory\n",
    "        self.memory = list()\n",
    "\n",
    "    def remember(self, m):\n",
    "        \n",
    "        self.memory.append(m)\n",
    "        if len(self.memory)>100:\n",
    "            self.memory = self.memory[-100:]\n",
    "\n",
    "    def random_access(self):\n",
    "        return self.memory[np.random.randint(len(self.memory))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN(Agent):\n",
    "    def __init__(self, epsilon = 0.1, memory_size=100, batch_size = 16, n_state=4):\n",
    "        super(DQN, self).__init__(epsilon = epsilon)\n",
    "\n",
    "        # Discount for Q learning\n",
    "        self.discount = 0.8 # previous : 0.99\n",
    "        \n",
    "        # number of state\n",
    "        self.n_state = n_state\n",
    "\n",
    "        # Memory\n",
    "        self.memory = Memory(memory_size)\n",
    "        \n",
    "        # Batch size when learning\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "        \n",
    "\n",
    "    def learned_act(self, s):\n",
    "        \n",
    "        pred_rwd_per_action = self.model.predict(s.reshape(1,9,9,self.n_state))\n",
    "        return np.argmax(pred_rwd_per_action)\n",
    "\n",
    "    def reinforce(self, s_, n_s_, a_, r_, game_over_):\n",
    "        # Two steps: first memorize the states, second learn from the pool\n",
    "       \n",
    "            \n",
    "        self.memory.remember([s_, n_s_, a_, r_, game_over_])\n",
    "        \n",
    "        input_states = np.zeros((self.batch_size, 9, 9, self.n_state))\n",
    "        target_q = np.zeros((self.batch_size, 4))\n",
    "        \n",
    "        for i in range(self.batch_size):\n",
    "            sample = self.memory.random_access()\n",
    "            \n",
    "            input_states[i,:,:,:] = sample[0]\n",
    "            \n",
    "            target_q[i, sample[2]] = sample[3]\n",
    "            \n",
    "            if not game_over_:\n",
    "                target_q[i, sample[2]] += self.discount*np.max(self.model.predict(sample[1].reshape(1,9,9,self.n_state)).flatten())\n",
    "        \n",
    "        target_q = np.clip(target_q, -30, 30)\n",
    "\n",
    "        l = self.model.train_on_batch(input_states, target_q)\n",
    "\n",
    "        return l\n",
    "\n",
    "    def save(self, name_weights='model.h5', name_model='model.json'):\n",
    "        self.model.save_weights('runs/'+name_weights, overwrite=True)\n",
    "        with open('runs/'+name_model, \"w\") as outfile:\n",
    "            json.dump(self.model.to_json(), outfile)\n",
    "            \n",
    "    def load(self,name_weights='model.h5',name_model='model.json'):\n",
    "        with open('runs/'+name_model, \"r\") as jfile:\n",
    "            model = model_from_json(json.load(jfile))\n",
    "        model.load_weights('runs/'+name_weights)\n",
    "        model.compile(\"sgd\", \"mse\")\n",
    "        self.model = model\n",
    "\n",
    "\n",
    "def my_custom_loss(y_true,y_pred):\n",
    "    \"\"\"\n",
    "    Let's have a look at the issue when using the MSE loss. Take for example the situation in which :\n",
    "       - Target = [   0, 0.8,    0,   0]\n",
    "       - Output = [-0.3, 0.1, -1.5, 4.1]\n",
    "    Then the MSE loss will also consider the distance between 0 and the conrresponding output value even though\n",
    "    0 means that we don't have the information since the action wasn't considered. Thus the algorithm is mislead\n",
    "    with a wrong information and it actually learns either to reproduce it's output, or to only output values\n",
    "    close to 0.\n",
    "    \n",
    "    What could be much more meaningful would be to only consider the squared deviation between the target and\n",
    "    the output but only for the information that we have.\n",
    "    \"\"\"\n",
    "    norm = K.sum(y_true, axis=-1)+1e-6 # Get the value of the non-zero element from the target\n",
    "    norm = K.reshape(norm, shape=(-1,1)) # Reshape it in order to concatenate it after\n",
    "    norm = K.concatenate([norm, norm, norm, norm],axis=-1) # Concatenate in order to allow the next operation\n",
    "    loss = K.sum(K.square(y_true * (y_true - y_pred) / norm), axis=-1) # Multiply the deviation by the target\n",
    "    # thus the non selected actions won't be considered in the loss (and divide by the norm of the target not \n",
    "    # to impact the final loss)\n",
    "    return loss\n",
    "\n",
    "class DQN_FC(DQN):\n",
    "    def __init__(self, *args, lr=0.1,**kwargs):\n",
    "        super(DQN_FC, self).__init__( *args,**kwargs)\n",
    "        \n",
    "        # NN Model\n",
    "        \n",
    "        model = Sequential()\n",
    "        model.add(Flatten())\n",
    "        model.add(Dense(32))\n",
    "        model.add(Dense(64))\n",
    "        model.add(Dense(4))\n",
    "        \n",
    "        loss = my_custom_loss #\"mse\"\n",
    "        optimizer = Adam(lr=lr) #sgd(lr=lr, decay=1e-4, momentum=0.0), loss=\"mse\")\n",
    "        \n",
    "        model.compile(optimizer=optimizer, loss=loss)\n",
    "        self.model = model\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K.clear_session()\n",
    "env = Environment(grid_size=size, max_time=T, temperature=0.3)\n",
    "agent = DQN_FC(lr=.0001, epsilon = 0.3, memory_size=2000, batch_size = 64)\n",
    "train(agent, env, epoch=20, prefix='fc_train')\n",
    "HTML(display_videos('fc_train20.mp4'))\n",
    "\n",
    "# we get impressively good results ! \n",
    "# It could be interesting to select the best set of parameters with a grid search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "What if we were using a CNN instead ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN_CNN(DQN):\n",
    "    def __init__(self, *args,lr=0.1,**kwargs):\n",
    "        super(DQN_CNN, self).__init__(*args,**kwargs)\n",
    "        \n",
    "        model = Sequential()\n",
    "        model.add(Conv2D(32, kernel_size=(3,3), activation='relu'))\n",
    "        model.add(Conv2D(64, kernel_size=(3,3), activation='relu'))\n",
    "        model.add(Flatten())\n",
    "        model.add(Dense(16))\n",
    "        model.add(Dense(4))\n",
    "        \n",
    "        loss = my_custom_loss #\"mse\"\n",
    "        optimizer = Adam(lr=lr) #sgd(lr=lr, decay=1e-4, momentum=0.0), loss=\"mse\")\n",
    "        \n",
    "        model.compile(optimizer=optimizer, loss=loss)\n",
    "        self.model = model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K.clear_session()\n",
    "env = Environment(grid_size=size, max_time=T, temperature=0.3)\n",
    "agent = DQN_CNN(lr=.001, epsilon = 0.2, memory_size=2000, batch_size = 64)\n",
    "train(agent, env, epoch=20, prefix='cnn_train')\n",
    "HTML(display_videos('cnn_train20.mp4'))\n",
    "\n",
    "### Comments :\n",
    "# The loss represents the learning of the algorithm and the win/lose count represents somehow how well does the\n",
    "# algorithm generalizes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CNN seems to perform better that FC, but we see that they both struggle to find new cheese at some point... Exploration seems to be a limitation of our approach..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "***\n",
    "Compare both algorithms' performances :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "K.clear_session()\n",
    "env = Environment(grid_size=size, max_time=T,temperature=0.3)\n",
    "agent_cnn = DQN_CNN(lr=.1, epsilon = 0.1, memory_size=2000, batch_size = 32)\n",
    "agent_cnn.load(name_weights='cnn_train_model.h5', name_model='cnn_train_model.json')\n",
    "\n",
    "agent_fc = DQN_FC(lr=.1, epsilon = 0.1, memory_size=2000, batch_size = 32)\n",
    "agent_fc.load(name_weights='fc_train_model.h5', name_model='fc_train_model.json')\n",
    "print('Test of the CNN')\n",
    "test(agent_cnn, env, epochs_test, prefix='cnn_test')\n",
    "print('Test of the FC')\n",
    "test(agent_fc, env, epochs_test, prefix='fc_test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HTML(display_videos('cnn_test10.mp4'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HTML(display_videos('fc_test10.mp4'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> The local loss we introduced overcomes the issues related to the MSE one. In fact this is questionable since the MSE tends to make the network reproduce what it has done and penalizes bad moves (0 is better than negative rewards), but in our case, if we give a good exploration parameter we authorize the model to perform badly at first in order to get some cases to be able to predict future rewards. Once it has learned how to predict future rewards, it is thought to behave well but then would it get that bad rewards are worse than 0 ? This is questionable. In our case at least, it seems to work impressively well in very few epochs !!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "The algorithm tends to not explore the map which can be an issue. We propose two ideas in order to encourage exploration:\n",
    "1. Incorporating a decreasing $\\epsilon$-greedy exploration. We use the method ```set_epsilon```\n",
    "2. Append via the environment a new state that describes if a cell has been visited or not\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training\n",
    "K.clear_session()\n",
    "env_eplore = Environment(grid_size=size, max_time=T, temperature=0.3, immob_penalty=0.1)\n",
    "agent = DQN_CNN(lr=.001, epsilon=0.3, memory_size=2000, batch_size=32, n_state=3)\n",
    "train(agent, env, epoch=20, eps_decay=0.5, eps_step=3, prefix='cnn_train')\n",
    "HTML(display_videos('cnn_train_explore_20.mp4'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation\n",
    "test(agent, env, epochs_test, prefix='cnn_test_explore')\n",
    "HTML(display_videos('cnn_test_explore10.mp4'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> This approach works impressively well ! Indeed there is a local minimum in our original approach which needed to be avoided : since the model itself tries to be confident about its prediction somehow, one can think of the situation in which the mouse goes back and forth on 2 possible positions. Indeed, it will predict that the expected future reward is 0, which is true since it doesn't explore anymore and thus with a current reward of 0 and a future reward of 0, the model is able to predict very precisely its future and thus is in a local minimum. We need to avoid this situation by forcing an exploration. We need to put a high exploration parameter at first and then decrease it when the model has enough data to train on. The other part of the new approach deals with giving a minus reward when coming back on visited locations, which is great since the model is not able to predict the future reward precisely (infinite instead of 0)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "---\n",
    "# Adaptive Environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To be tested :\n",
    "- I) Modify the environment itself :\n",
    "    - I-1) **Translation** of the board every T time step by a factor to be set as parameter\n",
    "    - I-2) **Flip** the board based on a central/axial symmetry every T time step.\n",
    "    - I-3) **Reset** to a random board at every T time step. (should break the algorithm and actually be pretty interesting considering the fact that the DQN approach with a CNN tries to be confident over its predictions... How can it be in this situation ?)\n",
    "    - I-3 bis) **Reinitialize** the board (reset with same initialization) : it should be a sanity check\n",
    "    - I-4) **Impact of the rat's actions** : based on what the rat eats, his vision can increase in dimension\n",
    "    - I-5) **Impact of the rat's actions** : new rewards pop on the board based on the rat's actions\n",
    "- II) Rewards follow a **distribution of probability** rather than being fixed : cheese ~ N(0.5, 1) | poison ~ N(-1, 0.5) for instance.\n",
    "- III) **Adverse environnement** (environnement acts against the player). Implement and explore the Exp3 algorithm and compare it to the UCB1 one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Adaptive_Environment(Environment):\n",
    "    def __init__(self, *args, grid_size=13, max_time=200, temperature=0.3, transform_board=None,\n",
    "                 transform_visibility=None, add_reward=None, **kwargs):\n",
    "        super(Adaptive_Environment, self).__init__(*args,grid_size=grid_size, max_time=max_time, \n",
    "                                                   temperature=temperature,**kwargs)\n",
    "        \n",
    "        self.transform_board = transform_board\n",
    "        self.transform_visibility = transform_visibility\n",
    "        self.add_reward = add_reward\n",
    "\n",
    "    def modify_board(self):\n",
    "        \"\"\"This function modifies the board itself. It should use self.t and self.board.\n",
    "        To do so, we consider the transform_board function passed to the adaptative environment.\n",
    "        Such a function should take as input the arguments of `self` and return the new board.\n",
    "        \"\"\"\n",
    "        if self.transform_board is not None:\n",
    "            self.board, self.malus_position = self.transform_board(infos=self)\n",
    "        else:\n",
    "            pass\n",
    "    \n",
    "    def modify_visibility(self,win,lose):\n",
    "        \"\"\"This function modifies the visibility area of the Agent. It should use self.vis.\n",
    "        To do so, we consider the transform_visibility function passed to the adaptative environment.\n",
    "        Such a function should take as input the arguments of `self` and return the new vis.\n",
    "        \"\"\"\n",
    "        if self.transform_visibility is not None:\n",
    "            self.vis = self.transform_visibility(infos=self,win=win,lose=lose)\n",
    "        else:\n",
    "            pass\n",
    "    \n",
    "    def add_big_reward(self,win):\n",
    "        \"\"\"This function modifies the visibility area of the Agent. It should use self.vis.\n",
    "        To do so, we consider the transform_visibility function passed to the adaptative environment.\n",
    "        Such a function should take as input the arguments of `self` and return the new vis.\n",
    "        \"\"\"\n",
    "        if self.add_reward is not None:\n",
    "            self.big_bonus = self.add_reward(infos=self,win=win)\n",
    "        else:\n",
    "            pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I - Modify the environment itself"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1 - Translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def board_translation(infos, time_step=2, amplitude=1):\n",
    "    \"\"\"Translate the board\n",
    "    \n",
    "    input :\n",
    "    -----\n",
    "        infos : parser\n",
    "            An object containing informations about the environment (it is the self object from the class)\n",
    "        time_step : int\n",
    "            Frequency at which we want to modify the board.\n",
    "        amplitude : int\n",
    "            Number of columns (or rows, it doesn't matter given the symmetry of the game) to consider\n",
    "            to shift the board.\n",
    "            \n",
    "    output :\n",
    "    ------\n",
    "        a new version of the board contained in the object self\n",
    "    \"\"\"\n",
    "    board = infos.board\n",
    "    malus_position = infos.malus_position\n",
    "    if infos.t%time_step==0: # done after self.t += 1 so the case t=0 doesn't need to be considered\n",
    "        # for instance with an amplitude of 2 : board[:, 2:] = board[:, :-2] | board[:, :2] = board[:, -2:]\n",
    "        board[:, amplitude:], board[:, :amplitude] = board[:, :-amplitude], board[:, -amplitude:]\n",
    "        malus_position[:, amplitude:], malus_position[:, :amplitude] = malus_position[:, :-amplitude], malus_position[:, -amplitude:]\n",
    "    return board, malus_position"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K.clear_session()\n",
    "##################################\n",
    "epoch = 10\n",
    "name = 'translate'\n",
    "transform_fct = board_translation\n",
    "##################################\n",
    "mod_env = Adaptive_Environment(grid_size=size, max_time=500, temperature=0.3, immob_penalty=0.1,\n",
    "                               transform_board=transform_fct)\n",
    "agent = DQN_CNN(lr=.001, epsilon=0.3, memory_size=2000, batch_size=32, n_state=3)\n",
    "train(agent, mod_env, epoch=epoch, eps_decay=0.5, eps_step=3, freq_video=1, prefix='{}_env'.format(name))\n",
    "HTML(display_videos('{}_env_explore_{}.mp4'.format(name, epoch)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2 - Random Flip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def board_flip(infos, time_step=10):\n",
    "    \"\"\"Translate the board\n",
    "    \n",
    "    input :\n",
    "    -----\n",
    "        infos : parser\n",
    "            An object containing informations about the environment (it is the self object from the class)\n",
    "        time_step : int\n",
    "            Frequency at which we want to modify the board.\n",
    "        amplitude : int\n",
    "            Number of columns (or rows, it doesn't matter given the symmetry of the game) to consider\n",
    "            to shift the board.\n",
    "            \n",
    "    output :\n",
    "    ------\n",
    "        a new version of the board contained in the object self\n",
    "    \"\"\"\n",
    "    board = infos.board\n",
    "    malus_position = infos.malus_position\n",
    "    if infos.t%time_step==0: # done after self.t += 1 so the case t=0 doesn't need to be considered\n",
    "        board = board[:, ::-1]\n",
    "        malus_position = malus_position[:, ::-1]\n",
    "    return board, malus_position"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K.clear_session()\n",
    "##################################\n",
    "epoch = 10\n",
    "name = 'flip'\n",
    "transform_fct = board_flip\n",
    "##################################\n",
    "mod_env = Adaptive_Environment(grid_size=size, max_time=500, temperature=0.3, immob_penalty=0.1,\n",
    "                               transform_board=transform_fct)\n",
    "agent = DQN_CNN(lr=.001, epsilon=0.3, memory_size=2000, batch_size=32, n_state=3)\n",
    "train(agent, mod_env, epoch=epoch, eps_decay=0.5, eps_step=3, freq_video=1, prefix='{}_env'.format(name))\n",
    "HTML(display_videos('{}_env_explore_{}.mp4'.format(name, epoch)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the case of time_step $\\neq 1$ is the approach using batches selected randomly in the memory relevant ? I don't think so but then how should we modify it ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3 - Reset the board"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def board_reset(infos, time_step=10):\n",
    "    \"\"\"Translate the board\n",
    "    \n",
    "    input :\n",
    "    -----\n",
    "        infos : parser\n",
    "            An object containing informations about the environment (it is the self object from the class)\n",
    "        time_step : int\n",
    "            Frequency at which we want to modify the board.\n",
    "        amplitude : int\n",
    "            Number of columns (or rows, it doesn't matter given the symmetry of the game) to consider\n",
    "            to shift the board.\n",
    "            \n",
    "    output :\n",
    "    ------\n",
    "        a new version of the board contained in the object self\n",
    "    \"\"\"\n",
    "    if infos.t%time_step==0: # done after self.t += 1 so the case t=0 doesn't need to be considered\n",
    "        malus_position = np.zeros((infos.grid_size, infos.grid_size))\n",
    "\n",
    "        bonus = 0.5*np.random.binomial(1, infos.temperature, size=infos.grid_size**2)\n",
    "        bonus = bonus.reshape(infos.grid_size, infos.grid_size)\n",
    "\n",
    "        malus = -1.0*np.random.binomial(1, infos.temperature, size=infos.grid_size**2)\n",
    "        malus = malus.reshape(infos.grid_size, infos.grid_size)\n",
    "\n",
    "        malus[bonus>0] = 0\n",
    "\n",
    "        board = bonus + malus\n",
    "    else:\n",
    "        board = infos.board\n",
    "        malus_position = infos.malus_position\n",
    "    return board, malus_position"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K.clear_session()\n",
    "##################################\n",
    "epoch = 10\n",
    "name = 'reset'\n",
    "transform_fct = board_reset\n",
    "##################################\n",
    "mod_env = Adaptive_Environment(grid_size=size, max_time=500, temperature=0.3, immob_penalty=0.1,\n",
    "                               transform_board=transform_fct)\n",
    "agent = DQN_CNN(lr=.001, epsilon=0.3, memory_size=2000, batch_size=32, n_state=3)\n",
    "train(agent, mod_env, epoch=epoch, eps_decay=0.5, eps_step=3, freq_video=1, prefix='{}_env'.format(name))\n",
    "HTML(display_videos('{}_env_explore_{}.mp4'.format(name, epoch)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**This last experiment is impressive !! 125.0+ | 11.0- it shows that the algorithm works impressively well at first and then struggle to find the final cheese. Here since the board is reset every 10 steps, the number of cheese is reset and the algorithm doesn't have to explore the whole board to get reward so it stays in place and eats every bonus in a certain global position. Great experiment !**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3 (bis) - Reset the board (same initialisation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def board_reinit(infos, time_step=10):\n",
    "    \"\"\"Translate the board\n",
    "    \n",
    "    input :\n",
    "    -----\n",
    "        infos : parser\n",
    "            An object containing informations about the environment (it is the self object from the class)\n",
    "        time_step : int\n",
    "            Frequency at which we want to modify the board.\n",
    "        amplitude : int\n",
    "            Number of columns (or rows, it doesn't matter given the symmetry of the game) to consider\n",
    "            to shift the board.\n",
    "            \n",
    "    output :\n",
    "    ------\n",
    "        a new version of the board contained in the object self\n",
    "    \"\"\"\n",
    "    if infos.t%time_step==0: # done after self.t += 1 so the case t=0 doesn't need to be considered\n",
    "        malus_position = np.zeros((infos.grid_size, infos.grid_size))\n",
    "\n",
    "        bonus = np.where(infos.to_draw[0, ::infos.scale, ::infos.scale, 0]==256, 0.5, 0)\n",
    "        malus = np.where(infos.to_draw[0, ::infos.scale, ::infos.scale, 2]==256, -1.0, 0)\n",
    "        \n",
    "        board = bonus + malus\n",
    "    else:\n",
    "        board = infos.board\n",
    "        malus_position = infos.malus_position\n",
    "    return board, malus_position"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K.clear_session()\n",
    "##################################\n",
    "epoch = 10\n",
    "name = 'reinit'\n",
    "transform_fct = board_reinit\n",
    "##################################\n",
    "mod_env = Adaptive_Environment(grid_size=size, max_time=500, temperature=0.3, immob_penalty=0.1,\n",
    "                               transform_board=transform_fct)\n",
    "agent = DQN_CNN(lr=.001, epsilon=0.3, memory_size=2000, batch_size=32, n_state=3)\n",
    "train(agent, mod_env, epoch=epoch, eps_decay=0.5, eps_step=3, freq_video=1, prefix='{}_env'.format(name))\n",
    "HTML(display_videos('{}_env_explore_{}.mp4'.format(name, epoch)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4) Impact of the rat's actions : based on what the rat eats, his vision can increase/decrease in dimension**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visibility_changed(infos,win,lose):\n",
    "    \"\"\"Increases the visibility of the rat when it has eaten enough cheese\n",
    "    \n",
    "    input :\n",
    "    -----\n",
    "        infos : parser\n",
    "            An object containing informations about the environment (it is the self object from the class)\n",
    "        \n",
    "            \n",
    "    output :\n",
    "    ------\n",
    "        a new version of the visibility contained in the object self\n",
    "    \"\"\"\n",
    "    vis = infos.vis\n",
    "    \n",
    "    if win%5==0 and vis < 4 and win>0:\n",
    "        vis = vis +1\n",
    "    elif lose%5==0 and vis>2 and lose>0 :\n",
    "        vis = vis - 1\n",
    "    return vis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K.clear_session()\n",
    "##################################\n",
    "epoch = 10\n",
    "name = 'visibility_changed'\n",
    "transform_fct = visibility_changed\n",
    "##################################\n",
    "mod_env = Adaptive_Environment(grid_size=size, max_time=500, temperature=0.3, immob_penalty=0.1,\n",
    "                               transform_visibility=transform_fct)\n",
    "agent = DQN_CNN(lr=.001, epsilon=0.3, memory_size=2000, batch_size=32, n_state=3)\n",
    "train(agent, mod_env, epoch=epoch, eps_decay=0.5, eps_step=3, freq_video=1, prefix='{}_env'.format(name))\n",
    "HTML(display_videos('{}_env_explore_{}.mp4'.format(name, epoch)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**5) Impact of the rat's actions : new rewards pop on the board based on the rat's actions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reward_changed(infos, win, value=2):\n",
    "    \"\"\"Increases the visibility of the rat when it has eaten enough cheese\n",
    "    \n",
    "    input :\n",
    "    -----\n",
    "        infos : parser\n",
    "            An object containing informations about the environment (it is the self object from the class)\n",
    "        \n",
    "            \n",
    "    output :\n",
    "    ------\n",
    "        a new version of the visibility contained in the object self\n",
    "    \"\"\"\n",
    "    big_bonus = infos.big_bonus\n",
    "    \n",
    "    if win%10==0 and win>0:\n",
    "        x_cheese, y_cheese = np.random.randint(4,infos.grid_size-4,2)\n",
    "        big_bonus[x_cheese, y_cheese]=value\n",
    "    return big_bonus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K.clear_session()\n",
    "##################################\n",
    "epoch = 10\n",
    "name = 'big_cheese_added'\n",
    "transform_fct = reward_changed\n",
    "##################################\n",
    "mod_env = Adaptive_Environment(grid_size=size, max_time=500, temperature=0.3, immob_penalty=0.1,\n",
    "                               add_reward=transform_fct)\n",
    "agent = DQN_CNN(lr=.001, epsilon=0.3, memory_size=2000, batch_size=32, n_state=4)\n",
    "train(agent, mod_env, epoch=epoch, eps_decay=0.5, eps_step=3, freq_video=1, prefix='{}_env'.format(name))\n",
    "HTML(display_videos('{}_env_explore_{}.mp4'.format(name, epoch)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### II - Probability distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def board_dist(infos):\n",
    "    \"\"\"Translate the board\n",
    "    \n",
    "    input :\n",
    "    -----\n",
    "        infos : parser\n",
    "            An object containing informations about the environment (it is the self object from the class)\n",
    "        time_step : int\n",
    "            Frequency at which we want to modify the board.\n",
    "        amplitude : int\n",
    "            Number of columns (or rows, it doesn't matter given the symmetry of the game) to consider\n",
    "            to shift the board.\n",
    "            \n",
    "    output :\n",
    "    ------\n",
    "        a new version of the board contained in the object self\n",
    "    \"\"\"\n",
    "    board = infos.board\n",
    "    malus_position = infos.malus_position\n",
    "    x = infos.x\n",
    "    y = infos.y\n",
    "    \n",
    "    if board[x,y]==0.5:\n",
    "        board[x,y] = np.random.normal(0.5,1.0)\n",
    "    elif board[x,y]==-1.0:\n",
    "        board[x,y] = np.random.normal(-1.,0.5)\n",
    "    \n",
    "    ### Should we consider the malus_position as being a random variable as well ?\n",
    "    #malus_position = infos.malus_position\n",
    "    return board, malus_position"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K.clear_session()\n",
    "##################################\n",
    "epoch = 10\n",
    "name = 'prob_dist'\n",
    "transform_fct = board_dist\n",
    "##################################\n",
    "mod_env = Adaptive_Environment(grid_size=size, max_time=500, temperature=0.3, immob_penalty=0.1,\n",
    "                               transform_board=transform_fct)\n",
    "agent = DQN_CNN(lr=.001, epsilon=0.3, memory_size=2000, batch_size=32, n_state=3)\n",
    "train(agent, mod_env, epoch=epoch, eps_decay=0.5, eps_step=3, freq_video=1, prefix='{}_env'.format(name))\n",
    "HTML(display_videos('{}_env_explore_{}.mp4'.format(name, epoch)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
