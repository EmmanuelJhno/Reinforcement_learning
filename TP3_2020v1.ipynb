{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.8"
    },
    "colab": {
      "name": "TP3_2020v1.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/EmmanuelJhno/Reinforcement_learning/blob/master/TP3_2020v1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rFta1S05nZWP",
        "colab_type": "text"
      },
      "source": [
        "# TP3: Model Free"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3lOJ0XEfnZWf",
        "colab_type": "text"
      },
      "source": [
        "### Description: \n",
        "\n",
        "In this session, we are exploring a simple version of a game\n",
        "(simpler version of Perudo: https://www.youtube.com/watch?v=die0n-eonl8).\n",
        "Using the rules of the game, we first construct an environment. \n",
        "  \n",
        "There is below a simple code where the game is played using a random statregy.\n",
        "\n",
        "Also, there are two functions to display the optimal value functions and optimal policies.\n",
        "\n",
        "\n",
        "### TO DO:\n",
        "\n",
        "1) Implement MC, SARSA, Q-learning to learn the value function. It is recommended to use the indications of code below.\n",
        "\n",
        "\n",
        "2) For the 3 cases, display the value function and the optimal policy found. \n",
        "\n",
        "3) Create a new environment which takes as parameter a given policy. Then implement iterations where you find the optimal value function for a given adversarial policy and then you play in turn against this policy.\n",
        "Display the results after some iterations. Comment."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p1MPxW-bnZWq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from IPython.display import Image\n",
        "from IPython.core.display import HTML \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6dM0xauHnZXO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import gym\n",
        "import numpy as np\n",
        "import random\n",
        "from gym import spaces\n",
        "from gym.utils import seeding"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "blgTzp6enZXj",
        "colab_type": "text"
      },
      "source": [
        "THE GAME"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oxlh_YVhnZXq",
        "colab_type": "text"
      },
      "source": [
        "**Rules**:\n",
        "\n",
        "- 2 players \n",
        "- Each player has 5 coins (head or tail). Each player only sees her coins.  \n",
        "- After flipping each coin, the game starts.\n",
        "- The game consists in guessing how many heads are present between all coins (or make the other player guess wrongly).\n",
        "- Bets start at 0 head.\n",
        "- The starting player is chosen at random. (Flip a coin)\n",
        "- Possible actions:\n",
        "    * the player keeps the actual bet and passes.\n",
        "    * the player add 1 to the actual bet (estimate of the total number of heads).\n",
        "- The game stops when one player passes.\n",
        "- if the bet is strictly bigger than the real number, the last player to play looses (r=-1) and the other wins (r=1). if the bet is smaller or equal, the last player wins (r=1) and the other looses (r=-1).\n",
        "- IA initialisation of the computer strategy: if the bet  is smaller than 2+ quantity of observed own heads, bets, otherwise passes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cMeeo5o6nZXw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Environment and rules\n",
        "\n",
        "def throw_coin(num_coin, np_random):\n",
        "    return np_random.rand(num_coin)>0.5\n",
        "\n",
        "def total_faces(list_players):\n",
        "    RV=0\n",
        "    for player in list_players:\n",
        "        RV += sum(player)\n",
        "    return RV\n",
        "\n",
        "class PerudoSimplificado(gym.Env):\n",
        "    def __init__(self):\n",
        "        \n",
        "        self.action_space = spaces.Discrete(2)\n",
        "        self.observation_space = spaces.Tuple((\n",
        "            spaces.Discrete(5), #mis monedas\n",
        "            spaces.Discrete(10))) #apuesta actual\n",
        "\n",
        "        self.seed()\n",
        "        # Empieza el juego\n",
        "        self.reset()\n",
        "\n",
        "    def seed(self, seed=None):\n",
        "        self.np_random, seed = seeding.np_random(seed)\n",
        "        return [seed]\n",
        "\n",
        "    def step(self, action):\n",
        "        assert self.action_space.contains(action)\n",
        "        \n",
        "        max_guess_player_2 = total_faces([self.player_2]) + int(len(self.player_1)/2)\n",
        "        faces_tot = total_faces([self.player_1, self.player_2])\n",
        "        \n",
        "        if self.guess > len(self.player_1) + len(self.player_2): #the bet is bigger than the max possible\n",
        "            done = True\n",
        "            reward = -1\n",
        "\n",
        "        if action == 0: #action == 0, maintain the bet and pass \n",
        "            done = True\n",
        "            if self.guess <= faces_tot: #the other player was right\n",
        "                reward = -1\n",
        "            else: #I was right\n",
        "                reward = -1\n",
        "            \n",
        "        else: #action == 1, add 1 in the bet \n",
        "            self.guess += 1\n",
        "            if self.guess < max_guess_player_2: # the other player adds 1\n",
        "                self.guess += 1 \n",
        "                done = False\n",
        "                reward = 0\n",
        "            else: # other player passes\n",
        "                done = True\n",
        "                if self.guess <= faces_tot: \n",
        "                    reward = 1\n",
        "                else:\n",
        "                    reward = -1\n",
        "            \n",
        "        return self.get_obs(), reward, done, {}\n",
        "\n",
        "    def get_obs(self):\n",
        "        return (sum(self.player_1), self.guess)\n",
        "    \n",
        "    def reset(self):\n",
        "        self.player_1 = throw_coin(5,self.np_random)\n",
        "        self.player_2 = throw_coin(5,self.np_random)\n",
        "        self.guess = 1 if np.random.rand()>0.5 else 0 # flip a coin to see who starts.\n",
        "        return self.get_obs()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IuIJendwnZYF",
        "colab_type": "text"
      },
      "source": [
        "## Playing at random"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qQEJ4OzdnZYK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "env = PerudoSimplificado()\n",
        "print(env.observation_space)\n",
        "print(env.action_space)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CphM8Lo_nZYb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Politica Random:\n",
        "for i_episode in range(5):\n",
        "    state = env.reset()\n",
        "    while True:\n",
        "        action = env.action_space.sample() # Selects a random action \n",
        "        state, reward, done, info = env.step(action) # Plays one round\n",
        "        print(state,action)\n",
        "        if done:\n",
        "            print('Game over! Your reward: ', reward)\n",
        "            print('You win :)\\n') if reward > 0 else print('You lost:(\\n')\n",
        "            break"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KrWKydwpnZYt",
        "colab_type": "text"
      },
      "source": [
        "## Graficos:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jsY2NDxWnZY2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "import matplotlib.pyplot as plt\n",
        "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
        "\n",
        "def plot_values(V):\n",
        "    \"\"\"\n",
        "    Does a 3D display of the value function.\n",
        "        \n",
        "    The parameter V describes the value function in function of the number of \"heads in your hand\"\n",
        "    and \"actual bet\".\n",
        "    \"\"\"\n",
        "    def get_Z(x, y):\n",
        "        if (x,y) in V:\n",
        "            return V[x,y]\n",
        "        else:\n",
        "            return 0\n",
        "\n",
        "    def get_figure(ax):\n",
        "        x_range = np.arange(0, 6)\n",
        "        y_range = np.arange(1, 11)\n",
        "        X, Y = np.meshgrid(x_range, y_range)\n",
        "        \n",
        "        Z = np.array([get_Z(x,y) for x,y in zip(np.ravel(X), np.ravel(Y))]).reshape(X.shape)\n",
        "\n",
        "        surf = ax.plot_surface(X, Y, Z, rstride=1, cstride=1, cmap=plt.cm.coolwarm, vmin=-1.0, vmax=1.0)\n",
        "        ax.set_xlabel('heads')\n",
        "        ax.set_ylabel('bets')\n",
        "        ax.set_zlabel('value')\n",
        "        ax.view_init(ax.elev, -120)\n",
        "    fig = plt.figure(figsize=(20, 20))\n",
        "    ax = fig.add_subplot(211, projection='3d')\n",
        "    get_figure(ax)\n",
        "    plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uKRHgC2onZZJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def plot_policy(policy):\n",
        "    \"\"\"\n",
        "    3D graphic of value function.\n",
        "    \n",
        "     policy is a function of \"heads\" \n",
        "     and \"bets\" and the value is the action to be realized.\n",
        "    \"\"\"\n",
        "    def get_Z(x, y):\n",
        "        if (x,y) in policy:\n",
        "            return policy[x,y]\n",
        "        else:\n",
        "            return 25 # this value is to vizualize that there is no action yet defined for this state\n",
        "             \n",
        "\n",
        "    def get_figure( ax):\n",
        "        x_range = np.arange(0, 6)\n",
        "        y_range = np.arange(0, 11)\n",
        "        X, Y = np.meshgrid(x_range, y_range)\n",
        "        Z = np.array([[get_Z(x,y) for x in x_range] for y in y_range])\n",
        "        surf = ax.imshow(np.flip(Z,0), cmap=plt.get_cmap('Pastel2', 3), vmin=0, vmax=2, extent=[-0.5, 5.5, -0.5, 10.5])\n",
        "        plt.xticks(x_range)\n",
        "        plt.yticks(y_range)\n",
        "        plt.gca().invert_yaxis()\n",
        "        ax.set_xlabel('heads')\n",
        "        ax.set_ylabel('bets')\n",
        "        ax.grid(color='w', linestyle='-', linewidth=1)\n",
        "        divider = make_axes_locatable(ax)\n",
        "        cax = divider.append_axes(\"right\", size=\"5%\", pad=0.1)\n",
        "        cbar = plt.colorbar(surf, ticks=[0,1,2], cax=cax)\n",
        "        cbar.ax.set_yticklabels(['0 (pass)','1 (up)', 'unknown'])\n",
        "        print(Z)\n",
        "            \n",
        "    fig = plt.figure(figsize=(5, 5))\n",
        "    ax = fig.add_subplot(111)\n",
        "    get_figure(ax)\n",
        "    plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XlQkQlwBnZZW",
        "colab_type": "text"
      },
      "source": [
        "## Monte Carlo"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qbAmEYVhnZZe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_probs(Q_s, epsilon, nA): \n",
        "    # Complete\n",
        "    return policy_s\n",
        "  \n",
        "\n",
        "def generate_episode_from_Q(env, Q, epsilon, nA):\n",
        "    # Complete\n",
        "    return episode\n",
        "\n",
        "\n",
        "def update_Q(env, episode, Q, alpha, gamma):\n",
        "    # Complete\n",
        "    return Q"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "srQO7bZxnZZy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def mc_control(env, num_episodes, alpha, gamma=1.0, eps_start=1.0, eps_decay=.99999, eps_min=0.05):\n",
        "    # Complete\n",
        "    return policy, Q"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xr9zcniMnZaW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Compute  the optimal policy and value function\n",
        "policy, Q = mc_control(env, 500000,0.015)\n",
        "V = dict((k,np.max(v)) for k, v in Q.items())\n",
        "plot_values(V)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NH0AjLahnZao",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# plot the policy\n",
        "plot_policy(policy)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GtjmKdD2nZa_",
        "colab_type": "text"
      },
      "source": [
        "## SARSA"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e7uiwTYhnZbC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def update_Q_sarsa(alpha, gamma, Q, state, action, reward, next_state=None, next_action=None):\n",
        "    # Complete\n",
        "    \n",
        "def epsilon_greedy(Q, state, nA, eps):\n",
        "    # Complete"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7KyuvL6onZbV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def sarsa(env, num_episodes, alpha, gamma=1.0, epsmin=0.01):\n",
        "    # Complete\n",
        "    return Q"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F3GpR4lSnZbk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Compute  the optimal policy and value function\n",
        "Q_sarsa = sarsa(env, 500000, 0.009)\n",
        "V = dict((k,np.max(v)) for k, v in Q_sarsa.items())\n",
        "plot_values(V)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MxsOj2FWnZbv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# plot\n",
        "policy_sarsa = dict((k,np.argmax(v)) for k, v in Q_sarsa.items())\n",
        "plot_policy(policy_sarsa)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "To_40kVlnZcC",
        "colab_type": "text"
      },
      "source": [
        "## Q-learning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LWgJkxVsnZcG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def update_Q_sarsamax(alpha, gamma, Q, state, action, reward, next_state=None):\n",
        "    # Complete\n",
        "    return new_value"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gdfJX4KynZcd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def q_learning(env, num_episodes, alpha, gamma=1.0,epsmin=0.01):\n",
        "    # Complete\n",
        "    return Q"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qXOxP3oMnZcu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Compute  the optimal policy and value function\n",
        "Q_sarsamax = q_learning(env, 500000, 0.01)\n",
        "V = dict((k,np.max(v)) for k, v in Q_sarsamax.items())\n",
        "plot_values(V)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aAq-CdkcnZc9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# plot\n",
        "policy_sarsamax = dict((k,np.argmax(v)) for k, v in Q_sarsamax.items())\n",
        "plot_policy(policy_sarsamax)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xiSL_AREnZdN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}